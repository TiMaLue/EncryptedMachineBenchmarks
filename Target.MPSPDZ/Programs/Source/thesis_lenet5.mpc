import tomlkit
import json

import numpy as np
import torch
from Compiler import ml
from Compiler.types import sfix, sint
from Models.LeNet5 import LeNet5
from torch.utils.data import Dataset, Subset
from torchvision import transforms
from torchvision.datasets import MNIST
from pathlib import Path

# load config
program.options_from_args()
# config.read("/scheduler/config.ini")
config_path = program.args[1]
print_ln("Reading config: %s", config_path)
with open(config_path, "rb") as fp:
  config = tomlkit.load(fp)

with open(program.args[2]) as scheduled_params_file:
    scheduled_params = json.load(scheduled_params_file)
print_ln("Scheduled params: %s", scheduled_params)

if scheduled_params["use_edabits"]:
    program.use_edabit(True)

# dataset_size = int(os.environ["DATASET_SIZE"])
# batch_size = int(os.environ["BATCH_SIZE"])
batch_size = scheduled_params["batch_size"]
dataset_size = scheduled_params["dataset_size"]
if "notorch" in program.args:
    # normalize to [0,1] before input
    samples = sfix.Tensor([dataset_size, 28, 28])
    labels = sint.Tensor([dataset_size, 1])
    samples.input_from(0)
    labels.input_from(0)

else:
    # data preprocessing
    ds = MNIST(
        root=Path(config["Dataset"]["RootDir"]),
        train=False,
        download=True,
        transform=transforms.ToTensor(),
    )
    random_seed = config["Dataset"]["RandomSeed"]
    print(f"Shuffling with random seed: {random_seed}")
    rng = np.random.default_rng(seed=random_seed)
    rng.shuffle(ds.data.numpy())
    rng.shuffle(ds.targets.numpy())
    indices = np.arange(dataset_size)
    partial_ds: Subset[Dataset[MNIST]] = Subset(ds, indices)
    partial_samples = ds.data[indices]
    partial_labels = ds.targets[indices]
    # normalize to [0,1] before input
    samples = sfix.input_tensor_via(0, partial_samples / 255, binary=True)
    labels = sint.input_tensor_via(0, partial_labels, one_hot=True, binary=True)

# load model and read into MP-SPDZ
lenet = LeNet5()
save_path = Path(config["Models"]["SaveDir"]).joinpath(
    "".join([lenet.model_name, config["Models"]["Postfix"]])
)
# use map_location to load models that were trained with CUDA/on GPU
lenet.load_state_dict(
    torch.load(save_path, weights_only=True, map_location=torch.device("cpu"))
)

# test model in plaintext first
test_loader = torch.utils.data.DataLoader(
    partial_ds, batch_size=batch_size, shuffle=False
)

# test loop
correct = 0
total = 0

with torch.no_grad():
    for plain_data in test_loader:
        # print(plain_data)
        plain_inputs, plain_labels = plain_data
        outputs = lenet(plain_inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += plain_labels.size(0)
        correct += (predicted == plain_labels).sum().item()

print_ln(f"PlaintextAcc:{correct / total}")

# scheduled_params = {
#     "layer_replacement": [
#         {
#             "layer_indices": np.arange(0, 10),
#             "layer_to_replace": "Sigmoid",
#             "replace_with": "Sigmoid3Piece",
#         }
#     ]
# }
# print_ln(f"scheduled_params: {scheduled_params}")

layers = ml.layers_from_torch(
    lenet, samples.shape, batch_size, scheduler_opts=scheduled_params, input_via=1
)
print_ln(f"Imported Layers: {layers}")

ml.set_n_threads(config["MP_SPDZ"]["NumThreads"])

optimizer = ml.Optimizer(layers)
n_correct, loss = optimizer.reveal_correctness(
    samples, labels, batch_size, running=True
)
accuracy = n_correct / len(samples)
print_ln("Accuracy:%s", accuracy)
print_ln("Loss:%s", loss)
print_ln("Correct:%s", n_correct)
# accuracy.print_plain()
# probabilities = optimizer.eval(samples)
# probabilities.print_reveal_nested(end='\n')
